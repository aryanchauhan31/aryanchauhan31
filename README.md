# ğŸ‘‹ Hi, I'm Aryan Chauhan

I'm a graduate student at **NYU** pursuing a Master's in Computer Engineering, passionate about building efficient and scalable AI systems. I focus on **LLM optimization**, **multimodal models**, and **open-source contributions**â€”most recently to the ğŸ¤— `transformers` library.

[![LinkedIn](https://img.shields.io/badge/LinkedIn-aryanchauhan31-blue?logo=linkedin)](https://linkedin.com/in/aryanchauhan31)
[![GitHub](https://img.shields.io/badge/GitHub-aryanchauhan31-black?logo=github)](https://github.com/aryanchauhan31)
[![Email](https://img.shields.io/badge/Email-ac11274@nyu.edu-red?logo=gmail)](mailto:ac11274@nyu.edu)

---

## ğŸ› ï¸ Technical Stack

- **Languages**: Python (PyTorch, NumPy, Scikit-Learn, PySpark, TensorFlow), C/C++, SQL, JavaScript
- **Domains**: LLMs, Vision-Language Models, Quantization, Distributed Training (DDP), Recommender Systems
- **Tools**: Docker, Slurm, Hugging Face Transformers, LangChain, Ollama, GCP, AWS, Spark, Airflow

---

## ğŸš€ Recent Work

### ğŸ¤– `transformers` Contributions
- **[Enable `device_map="auto"` support for Dinov2](https://github.com/huggingface/transformers/pull/XYZ)** â€“ Added `_no_split_modules` logic for efficient multi-GPU inference.

### ğŸ“¸ Multimodal VQA Optimization
- Developed ViT+BERT architecture for Visual Question Answering.
- Trained with QAT + DDP over 4Ã—L4 GPUs for 1.8Ã— speed-up and 60% model compression.

### ğŸ§  DistilBERT Compression Pipeline
- Reduced size by 64% with dynamic quantization.
- Automated finetuning and benchmarking via Hugging Face tools.

---

## ğŸ“ˆ What I'm Looking For
I'm currently open to:
- Remote internships or research collaborations in **LLM efficiency**, **model compression**, or **AI infrastructure**
- Open-source projects focused on cutting-edge ML research

---

## ğŸ“« Contact
- ğŸ“§ Email: [ac11274@nyu.edu](mailto:ac11274@nyu.edu)
- ğŸŒ Portfolio: [github.com/aryanchauhan31](https://github.com/aryanchauhan31)

---

*Letâ€™s build something impactful together.*
